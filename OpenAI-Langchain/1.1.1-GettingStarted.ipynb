{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting started With Langchain And Open AI\n",
    "\n",
    "In this quickstart we'll see how to:\n",
    "\n",
    "- Get setup with LangChain, LangSmith and LangServe\n",
    "- Use the most basic and common components of LangChain: prompt templates, models, and output parsers.\n",
    "- Build a simple application with LangChain\n",
    "- Trace your application with LangSmith\n",
    "- Serve your application with LangServe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv \n",
    "load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_KEY']=os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ['LANGCHAIN_API_KEY']= os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ['LANGCHAIN_TRACING_V2']=\"true\"\n",
    "os.environ['LANGCHAIN_PROJECT']=os.getenv(\"LANGCHAIN_Project\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.completions.Completions object at 0x000001F3893A17F0> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001F3895921E0> root_client=<openai.OpenAI object at 0x000001F38903D370> root_async_client=<openai.AsyncOpenAI object at 0x000001F389375070> model_name='gpt-4o' model_kwargs={} openai_api_key=SecretStr('**********')\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm=ChatOpenAI(model=\"gpt-4o\")\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=llm.invoke(\"what is generative AI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Generative AI refers to a category of artificial intelligence systems designed to generate new content. This can include text, images, music, and other forms of media, often by learning existing patterns and structures within input data. Unlike traditional AI, which focuses on task-specific predictions and classifications, generative AI creates new instances that share similar characteristics to the data it was trained on.\\n\\nSome common types of generative AI include:\\n\\n1. **Generative Adversarial Networks (GANs):** These consist of two neural networks (a generator and a discriminator) that are pitted against each other. The generator creates fake data, while the discriminator tries to distinguish between real and fake data. Over time, the generator improves its ability to produce data indistinguishable from real data.\\n\\n2. **Variational Autoencoders (VAEs):** These are used to compress and then reconstruct data, which is particularly useful for generating outputs similar to the trained input data.\\n\\n3. **Transformer Models:** These include well-known models like GPT (Generative Pre-trained Transformer), which are particularly effective in producing human-like text based on given prompts.\\n\\nGenerative AI has a wide range of applications, including content creation for marketing, drug discovery through molecular design, game development, and creating realistic simulations for various industries. However, it also raises ethical and societal questions, particularly concerning the potential for creating highly convincing fake media, called deepfakes, and issues related to intellectual property and authenticity.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 293, 'prompt_tokens': 12, 'total_tokens': 305, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_07871e2ad8', 'id': 'chatcmpl-BhJjV2oPi5KhgSbfFkRSrryTpVe7t', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--329025e3-a91b-4cd0-8df0-36f4f962e643-0' usage_metadata={'input_tokens': 12, 'output_tokens': 293, 'total_tokens': 305, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are an expert AI Engineer. Provide me answers based on the questions')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Chatprompt Template\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are an expert AI Engineer. Provide me answers based on the questions\"),\n",
    "        (\"user\",\"{input}\")\n",
    "    ]\n",
    "\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer. Provide me answers based on the questions'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are an expert AI Engineer. Provide me answers based on the questions\"),\n",
    "        (\"user\",\"{input}\")\n",
    "    ]\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Langsmith is a tool developed by LangChain, designed to assist developers in debugging, testing, and monitoring applications that use large language models (LLMs). It provides functionalities for prompt engineering and evaluation, facilitates real-time monitoring, and supports complex workflows with advanced features like tracing. Langsmith is especially useful for iterative development, allowing developers to optimize prompt quality and model performance, both during testing and in production environments. It can work seamlessly with LangChain and other ecosystem tools, offering a comprehensive solution for managing LLM applications.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 104, 'prompt_tokens': 32, 'total_tokens': 136, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_07871e2ad8', 'id': 'chatcmpl-BhJp0Vp0iHG9fCcls5H65oYeg0q2N', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--98f2adbf-3bdf-4bda-8063-16bd894deca6-0' usage_metadata={'input_tokens': 32, 'output_tokens': 104, 'total_tokens': 136, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "chain=prompt|llm\n",
    "\n",
    "response=chain.invoke({\"input\":\"can you tell me about Langsmith\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.messages.ai.AIMessage"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! As of my last update, Langsmith is a tool developed by LangChain that is designed for debugging and evaluating large language model (LLM) applications. It offers a suite of features to enhance the development and deployment process of these applications. Key aspects of Langsmith include:\n",
      "\n",
      "1. **Dataset Creation**: Users can create datasets for testing and evaluation purposes. This allows for the systematic evaluation of LLM applications with a variety of inputs to ensure reliability and performance.\n",
      "\n",
      "2. **Tracing and Debugging**: Langsmith provides mechanisms to trace the execution of applications built with LangChain frameworks. This tracing capability helps developers understand the decision-making processes of LLMs, identify issues, and optimize workflows.\n",
      "\n",
      "3. **Evaluation and Metrics**: The platform offers tools for evaluating the outputs of language models. Developers can measure performance using custom metrics, which is critical for refining models and ensuring they meet quality standards.\n",
      "\n",
      "4. **Interactive Interface**: Langsmith includes a user-friendly dashboard that enables developers to interact with datasets, visualize outputs, and debug various elements of their applications. This interactivity aids in iterative development and fine-tuning.\n",
      "\n",
      "Langsmith aims to simplify the complex processes involved in working with large language models and to provide a robust infrastructure for building scalable and efficient LLM applications. Please note that specific details or additional features might have been added since my last update.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "output_parser=StrOutputParser()\n",
    "chain=prompt|llm|output_parser\n",
    "\n",
    "response=chain.invoke({\"input\":\"can you tell me about Langsmith?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
